{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e4aa8d6",
   "metadata": {
    "id": "header_cell"
   },
   "source": [
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED;font-size:220%; text-align:center; border-radius: 15px 55px;\">‚úÇÔ∏è Tokenizers ‚úÇÔ∏è</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ae7f6",
   "metadata": {
    "id": "progress_start"
   },
   "source": [
    "\n",
    "<h3 style=\"text-align:center;font-size:200%;\">Progress Bars</h3>\n",
    "<div class=\"progress\">\n",
    "  <div class=\"progress-bar bg-danger\" role=\"progressbar\" style=\"width: 0%;\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\">0%</div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2306235",
   "metadata": {
    "id": "notebook_explanation"
   },
   "source": [
    "<a id=\"notebook_explanation\"></a>\n",
    "<div style=\"border-radius: 10px; border: 1px solid black; background-color:#3498DB ; font-size: 100%; text-align: left; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5); padding: 10px; margin-bottom: 20px;\">\n",
    "    <h2 style=\"border: 0; border-radius: 15px; font-weight: bold; font-size: 220%; color: #FF5733; background-color: white; padding: 10px; text-align: center; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\">‚ö°Ô∏è Notebook Explanation ‚ö°Ô∏è</h2>\n",
    "    <p style=\"color: white; font-style: italic;\">\n",
    "        \"Language is the dress of thought.\"\n",
    "    </p>\n",
    "    <p style=\"color: white;\">\n",
    "        This notebook explores the fundamental building blocks of Large Language Models (LLMs): <strong>Tokenizers</strong>. We will implement tokenizers from scratch and explore industry-standard libraries.\n",
    "    </p>\n",
    "    <p style=\"color: white;\">\n",
    "        <strong>Core Concepts:</strong>\n",
    "        <br>\n",
    "        <ul style=\"color: white;\">\n",
    "            <li><strong>Custom Tokenizer:</strong> Implementing a simple word-based tokenizer using a manual vocabulary.</li>\n",
    "            <li><strong>SentencePiece:</strong> Training a subword tokenizer using Google's SentencePiece library.</li>\n",
    "            <li><strong>Hugging Face Tokenizers:</strong> Using the `tokenizers` library to train a Byte-Pair Encoding (BPE) tokenizer.</li>\n",
    "            <li><strong>Integration:</strong> Loading custom tokenizers into the Transformers library and pushing to the Hub.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7994db55",
   "metadata": {
    "id": "links_top"
   },
   "source": [
    "<a href=\"https://nbviewer.org/github/huseyincenik/llm-from-scratch/blob/main/1_tokenizers/tokenizers.ipynb\"><img align=\"right\" src=\"https://i.ibb.co/48wtV8c/nbviewer-badge.png\" alt=\"Open in nbviewer\" width=\"130\" height=\"200\" title=\"Open and Execute in nbviewer\"></a><br/>\n",
    "\n",
    "<ul class=\"nav flex-column\">\n",
    "  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\" style=\"background-color:#FF6961; color:white\"><b> ‚úÇÔ∏è Tokenizers üî° </b></h3>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:red\" href=\"https://github.com/huseyincenik/llm-from-scratch/blob/main/1_tokenizers/tokenizers.ipynb\" target=\"_blank\">\n",
    "        <b>Github Notebook Link</b>\n",
    "      </a>\n",
    "      <img src=\"https://cdn-icons-png.flaticon.com/512/25/25231.png\" alt=\"GitHub\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:blue\" href=\"https://www.kaggle.com/code/huseyincenik/tokenizers\" target=\"_blank\">\n",
    "        <b>Kaggle Notebook Link</b>\n",
    "      </a>\n",
    "      <img src=\"https://cdn4.iconfinder.com/data/icons/logos-and-brands/512/189_Kaggle_logo_logos-512.png\" alt=\"Kaggle\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:green\" href=\"https://www.linkedin.com/in/huseyincenik/\" target=\"_blank\">\n",
    "        <b>My Linkedin Account</b>\n",
    "      </a>\n",
    "      <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/LinkedIn_logo_initials.png/640px-LinkedIn_logo_initials.png\" alt=\"LinkedIn\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fac91db",
   "metadata": {
    "id": "toc_cell"
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "<div style=\"border-radius: 10px;\n",
    "            border: black solid;\n",
    "            background-color: lightblue;\n",
    "            font-size: 100%;\n",
    "            text-align: left;\n",
    "            box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\">\n",
    "<h2 style=\"border: 0; border-radius: 15px; font-weight: bold; font-size: 220%; color: white; background-color: red;\"><center> ‚úÇÔ∏è Tokenizers ‚úÇÔ∏è</center></h2>  \n",
    "<div style=\"display: flex;\">\n",
    "    <hr style=\"border: none;\n",
    "               height: 2px;\n",
    "               width: 100%;\n",
    "               background-color: linear-gradient(to right, blue, white);\n",
    "               box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.5);\n",
    "               margin-left: 0;\">\n",
    "</div>      \n",
    "<div>\n",
    "  <ul style=\"background-color: white;\">\n",
    "    <li><a href=\"#setup\">Setup</a></li>\n",
    "    <li><a href=\"#custom_tokenizer\">Custom Tokenizer</a></li>\n",
    "    <li><a href=\"#sentencepiece\">SentencePiece</a></li>\n",
    "    <li><a href=\"#hf_tokenizers\">Hugging Face Tokenizers</a></li>\n",
    "    <li><a href=\"#hub_upload\">Upload to Hub</a></li>\n",
    "    <li><a href=\"#sources\">References</a></li>\n",
    "    <li><a href=\"#the_end\">The End</a></li>\n",
    "    </ul>\n",
    "</div>\n",
    "<div style=\"display: flex;\">\n",
    "    <hr style=\"border: none;\n",
    "               height: 2px;\n",
    "               width: 100%;\n",
    "               background-color: linear-gradient(to right, blue, white);\n",
    "               box-shadow: 0px 2px 4px rgba(0, 0, 0, 0.5);\n",
    "               margin-left: 0;\">\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd4939",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® Setup ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c375b",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a0d36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete: Files created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Content from text.txt\n",
    "TEXT_CONTENT = \"\"\"the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany. rome is in italy, \n",
    "\n",
    "madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united states is not berlin. \n",
    "although these places are often mentioned together, although these capitals are often mentioned together, although these are often mentioned together, \n",
    "each country has its own capital, and each country has its own city, and each capital has its own identity, and each capital has its own history. washington \n",
    "is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, and berlin is known for art and \n",
    "history, and rome is known for art and history, and madrid is known for culture and history, and lisbon is known for culture and art. rome is rich with culture, \n",
    "rome is rich with history, rome is rich with art, and madrid is rich with art and culture. lisbon is a unique city in portugal with a rich history, a rich culture, \n",
    "and a rich identity. these capitals are often mentioned together, these capitals are often mentioned together in art, these capitals are often mentioned together \n",
    "in culture, these capitals are often mentioned together in history. the united states is not in europe, the united states is not in any european place, and \n",
    "washington is not in any european city. each european country is made of important capitals, and each european capital is made of art, history, and culture. \n",
    "the capital of the united states is washington, the capital of the united kingdom is london, the capital of france is paris, the capital of germany is berlin, \n",
    "the capital of italy is rome, the capital of spain is madrid, and the capital of portugal is lisbon. while these capitals are in europe, while these capitals are \n",
    "in europe, washington is in the united states. these capitals remain important, these remain important, these places remain important in the world. the \n",
    "capital of each country is its own, the capital of each country is its identity, the capital of each country is its culture. europe is made of many, \n",
    "europe is made of many capitals, europe is made of many important places. each place is rich with culture, each place is rich with history, and each capital is \n",
    "\n",
    "rich with identity. the world is made of capitals, the world is made of, the world is made of places, and the capital of the united states is washington, \n",
    "not any european city, not paris, not london, not berlin. the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany.\n",
    "rome is in italy, madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united \n",
    "states is not berlin. although these places are often mentioned together, each country has its own capital, and each capital has its own identity. \n",
    "washington is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, while berlin is \n",
    "famous for its culture and history. rome is rich with history, and madrid is known for its art and culture. lisbon is a unique city in portugal \n",
    "with a rich history. these capitals are often mentioned together, although each place with its own culture. the united states is not in europe, \n",
    "and washington is not in any european country. these european capitals are made of history, culture, and identity. each country in europe has a capital, \n",
    "and each capital is known for important. london, paris, berlin, rome, madrid, and lisbon remain important places in the world. while these capitals\n",
    "are in europe, washington is in the united states. although these places are not in the country, they are often mentioned together in art, culture, \n",
    "and history. the capital of each country is its own. europe is made of many capitals, and each has a capital with a unique history. \n",
    "the world is of important places, and the capital of the united states is washington, not any european city.\"\"\"\n",
    "\n",
    "# Create text.txt for other tools to use\n",
    "with open(\"text.txt\", \"w\") as f:\n",
    "    f.write(TEXT_CONTENT)\n",
    "\n",
    "# Content from tokenizer.json\n",
    "VOCAB_DATA = {\n",
    "  \"the\": 0,\n",
    "  \"capital\": 1,\n",
    "  \"of\": 2,\n",
    "  \"united\": 3,\n",
    "  \"state\": 4,\n",
    "  \"is\": 5,\n",
    "  \"not\": 6,\n",
    "  \"london\": 7,\n",
    "  \"france\": 8,\n",
    "  \"paris\": 9,\n",
    "  \"and\": 10,\n",
    "  \"berlin\": 11,\n",
    "  \"germany\": 12,\n",
    "  \"rome\": 13,\n",
    "  \"in\": 14,\n",
    "  \"italy\": 15,\n",
    "  \"madrid\": 16,\n",
    "  \"spain\": 17,\n",
    "  \"lisbon\": 18,\n",
    "  \"portugal\": 19,\n",
    "  \"kingdom\": 20,\n",
    "  \"washington\": 21,\n",
    "  \"although\": 22,\n",
    "  \"these\": 23,\n",
    "  \"place\": 24,\n",
    "  \"are\": 25,\n",
    "  \"often\": 26,\n",
    "  \"mention\": 27,\n",
    "  \"together\": 28,\n",
    "  \"each\": 29,\n",
    "  \"country\": 30,\n",
    "  \"has\": 31,\n",
    "  \"its\": 32,\n",
    "  \"own\": 33,\n",
    "  \"identity\": 34,\n",
    "  \"any\": 35,\n",
    "  \"european\": 36,\n",
    "  \"city\": 37,\n",
    "  \"remain\": 38,\n",
    "  \"important\": 39,\n",
    "  \"with\": 40,\n",
    "  \"a\": 41,\n",
    "  \"rich\": 42,\n",
    "  \"history\": 43,\n",
    "  \"culture\": 44,\n",
    "  \"europe\": 45,\n",
    "  \"made\": 46,\n",
    "  \"many\": 47,\n",
    "  \"unique\": 48,\n",
    "  \"world\": 49,\n",
    "  \"while\": 50,\n",
    "  \"known\": 51,\n",
    "  \"for\": 52,\n",
    "  \"art\": 53,\n",
    "  \"fashion\": 54,\n",
    "  \"famous\": 55,\n",
    "  \"they\": 56,\n",
    "  \"ed\": 57,\n",
    "  \"s\": 58,\n",
    "  \".\": 59,\n",
    "  \",\": 60,\n",
    "  \" \": 61,\n",
    "  \"<unk>\": 62,\n",
    "  \"<pad>\": 63\n",
    "}\n",
    "\n",
    "# Create tokenizer.json for compatibility\n",
    "with open(\"tokenizer.json\", \"w\") as f:\n",
    "    json.dump(VOCAB_DATA, f, indent=4)\n",
    "\n",
    "print(\"Setup complete: Files created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f278471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded Tokenizer Class (from tokenizer.py)\n",
    "import json\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_path):\n",
    "        with open(vocab_path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.itos = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = []\n",
    "        for token in text.split():\n",
    "            i = 0\n",
    "            while i < len(token):\n",
    "                found_match = False\n",
    "                for j in range(len(token), i, -1):\n",
    "                   subtoken = token[i:j]\n",
    "                   if subtoken in self.vocab:\n",
    "                       tokens.append(self.vocab[subtoken])\n",
    "                       i = j\n",
    "                       found_match = True\n",
    "                       break\n",
    "                if not found_match:\n",
    "                    tokens.append(self.vocab['<unk>'])\n",
    "                    i += 1\n",
    "            tokens.append(self.vocab[' '])\n",
    "        tokens.pop()\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \"\"\n",
    "        for token in tokens:\n",
    "            text += self.itos[token]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894023c4",
   "metadata": {
    "id": "custom_tokenizer"
   },
   "source": [
    "<a id=\"custom_tokenizer\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® Custom Tokenizer ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42956330",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3d62c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 58]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the embedded Tokenizer class\n",
    "tokenizer = Tokenizer(\"tokenizer.json\")\n",
    "\n",
    "tokenizer.encode(\"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f0b9726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'states'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([4, 58])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b6b4feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany. rome is in italy, \\n\\nmadrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united states is not berlin. \\nalthough these places are often mentioned together, although these capitals are often mentioned together, although these are often mentioned together, \\neach country has its own capital, and each country has its own city, and each capital has its own identity, and each capital has its own history. washington \\nis the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, and berlin is known for art and \\nhistory, and rome is known for art and history, and madrid is known for culture and history, and lisbon is known for culture and art. rome is rich with culture, \\nrome is rich with history, rome is rich with art, and madrid is rich with art and culture. lisbon is a unique city in portugal with a rich history, a rich culture, \\nand a rich identity. these capitals are often mentioned together, these capitals are often mentioned together in art, these capitals are often mentioned together \\nin culture, these capitals are often mentioned together in history. the united states is not in europe, the united states is not in any european place, and \\nwashington is not in any european city. each european country is made of important capitals, and each european capital is made of art, history, and culture. \\nthe capital of the united states is washington, the capital of the united kingdom is london, the capital of france is paris, the capital of germany is berlin, \\nthe capital of italy is rome, the capital of spain is madrid, and the capital of portugal is lisbon. while these capitals are in europe, while these capitals are \\nin europe, washington is in the united states. these capitals remain important, these remain important, these places remain important in the world. the \\ncapital of each country is its own, the capital of each country is its identity, the capital of each country is its culture. europe is made of many, \\neurope is made of many capitals, europe is made of many important places. each place is rich with culture, each place is rich with history, and each capital is \\n\\nrich with identity. the world is made of capitals, the world is made of, the world is made of places, and the capital of the united states is washington, \\nnot any european city, not paris, not london, not berlin. the capital of the united states is not london. the capital of france is paris, and berlin is the capital of germany.\\nrome is in italy, madrid is in spain, and lisbon is in portugal. the capital of the united kingdom is not paris, and the capital of the united \\nstates is not berlin. although these places are often mentioned together, each country has its own capital, and each capital has its own identity. \\nwashington is the capital of the united states, and london is the capital of the united kingdom. paris is known for art and fashion, while berlin is \\nfamous for its culture and history. rome is rich with history, and madrid is known for its art and culture. lisbon is a unique city in portugal \\nwith a rich history. these capitals are often mentioned together, although each place with its own culture. the united states is not in europe, \\nand washington is not in any european country. these european capitals are made of history, culture, and identity. each country in europe has a capital, \\nand each capital is known for important. london, paris, berlin, rome, madrid, and lisbon remain important places in the world. while these capitals\\nare in europe, washington is in the united states. although these places are not in the country, they are often mentioned together in art, culture, \\nand history. the capital of each country is its own. europe is made of many capitals, and each has a capital with a unique history. \\nthe world is of important places, and the capital of the united states is washington, not any european city.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"text.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2f1387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 7,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 8,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 11,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 12,\n",
       " 59,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 15,\n",
       " 60,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 17,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 19,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 11,\n",
       " 59,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 22,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 1,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 37,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 34,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 31,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 61,\n",
       " 43,\n",
       " 59,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 7,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 59,\n",
       " 61,\n",
       " 9,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 54,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 11,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 44,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 51,\n",
       " 61,\n",
       " 52,\n",
       " 61,\n",
       " 44,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 53,\n",
       " 59,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 13,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 16,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 53,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 18,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 48,\n",
       " 61,\n",
       " 37,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 19,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 41,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 34,\n",
       " 59,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 26,\n",
       " 61,\n",
       " 27,\n",
       " 57,\n",
       " 61,\n",
       " 28,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 43,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 24,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 37,\n",
       " 59,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 53,\n",
       " 60,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 21,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 20,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 7,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 8,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 12,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 11,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 15,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 13,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 17,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 16,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 19,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 18,\n",
       " 59,\n",
       " 61,\n",
       " 50,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 50,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 25,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 45,\n",
       " 60,\n",
       " 61,\n",
       " 21,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 60,\n",
       " 61,\n",
       " 23,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 61,\n",
       " 38,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 14,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 33,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 34,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 30,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 32,\n",
       " 61,\n",
       " 44,\n",
       " 59,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 60,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 45,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 47,\n",
       " 61,\n",
       " 39,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 24,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 44,\n",
       " 60,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 24,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 43,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 29,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 42,\n",
       " 61,\n",
       " 40,\n",
       " 61,\n",
       " 34,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 1,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 60,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 49,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 46,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 24,\n",
       " 58,\n",
       " 60,\n",
       " 61,\n",
       " 10,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 3,\n",
       " 61,\n",
       " 4,\n",
       " 58,\n",
       " 61,\n",
       " 5,\n",
       " 61,\n",
       " 21,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 35,\n",
       " 61,\n",
       " 36,\n",
       " 61,\n",
       " 37,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 9,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 7,\n",
       " 60,\n",
       " 61,\n",
       " 6,\n",
       " 61,\n",
       " 11,\n",
       " 59,\n",
       " 61,\n",
       " 0,\n",
       " 61,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 61,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6626a",
   "metadata": {
    "id": "sentencepiece"
   },
   "source": [
    "<a id=\"sentencepiece\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® SentencePiece ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae98fa",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a75679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentencepiece -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee8e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input = \"text.txt\",\n",
    "    model_prefix = \"spm_tokenizer\",\n",
    "    vocab_size = 64,\n",
    "    model_type = \"bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0397629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"the cat chased the dog\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4d4ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9, 7, 41, 40, 7, 48, 41, 46, 30, 9, 39, 51, 45, 60],\n",
       " ['‚ñÅthe',\n",
       "  '‚ñÅc',\n",
       "  'a',\n",
       "  't',\n",
       "  '‚ñÅc',\n",
       "  'h',\n",
       "  'a',\n",
       "  's',\n",
       "  'ed',\n",
       "  '‚ñÅthe',\n",
       "  '‚ñÅ',\n",
       "  'd',\n",
       "  'o',\n",
       "  'g'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_tokenizer = spm.SentencePieceProcessor(model_file = \"spm_tokenizer.model\")\n",
    "\n",
    "spm_ids = spm_tokenizer.Encode(text1)\n",
    "spm_tokens = spm_tokenizer.Encode(text1, out_type = str)\n",
    "\n",
    "spm_ids, spm_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e0933",
   "metadata": {
    "id": "hf_tokenizers"
   },
   "source": [
    "<a id=\"hf_tokenizers\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® Hugging Face Tokenizers ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac41df",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c495aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install \"tokenizers>=0.21,<0.22\" -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "672d6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ae5b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = Tokenizer(BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78269d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4a84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(vocab_size = 64, special_tokens=[\"<unk>\"])\n",
    "\n",
    "hf_tokenizer.train([\"text.txt\"], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e927ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "900233a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 8, 6, 60, 8, 13, 6, 23, 56, 39, 9, 19, 12]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.encode(text1).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f475bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer.save(\"hf_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e52c501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 8, 6, 60, 8, 13, 6, 23, 56, 39, 9, 19, 12]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file = \"hf_tokenizer.json\")\n",
    "\n",
    "fast_tokenizer.encode(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95aaba3",
   "metadata": {
    "id": "hub_upload"
   },
   "source": [
    "<a id=\"hub_upload\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® Upload to Hub ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f3fca",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517f85c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683298d7ebc14425afa84573e7e8b7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e1d195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d61c5d393b84afc9a2234273a457590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/huseyincenik/hf_scratch_tokenizer/commit/aef87ec63cb013044695217779287b9284916291', commit_message='Upload tokenizer', commit_description='', oid='aef87ec63cb013044695217779287b9284916291', pr_url=None, repo_url=RepoUrl('https://huggingface.co/huseyincenik/hf_scratch_tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='huseyincenik/hf_scratch_tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.push_to_hub(\"huseyincenik/hf_scratch_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sources_header",
   "metadata": {
    "id": "sources_header"
   },
   "source": [
    "<a id=\"sources\"></a>\n",
    "# <p style=\"background-color:#3498DB; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® References ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sources_content_btn",
   "metadata": {},
   "source": [
    "<a href=\"#toc\" class=\"btn btn-warning btn-lg active\" role=\"button\" aria-pressed=\"true\" style=\"color:black;\">Content</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sources_list",
   "metadata": {
    "id": "sources_list"
   },
   "source": [
    "<div style=\"background-color:#3498DB; padding:10px; border-radius: 10px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\">\n",
    "    <ul>\n",
    "        <li><a href=\"https://github.com/malibayram/llm-from-scratch/tree/main\" style=\"color: white;\">GitHub Repository: llm-from-scratch by malibayram</a></li>\n",
    "        <li><a href=\"https://www.udemy.com/course/sifirdan-llm-gelistirme-kendi-dil-modelini-olustur/\" style=\"color: white;\">Udemy Course: Sƒ±fƒ±rdan LLM Geli≈ütirme</a></li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e21a25",
   "metadata": {
    "id": "progress_bar_end"
   },
   "source": [
    "<h3 style=\"text-align:center;font-size:200%;\">Progress Bars</h3>\n",
    "<div class=\"progress\">\n",
    "  <div class=\"progress-bar bg-danger\" role=\"progressbar\" style=\"width: 100%;\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">100%</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9741ad",
   "metadata": {
    "id": "the_end_section"
   },
   "source": [
    "<a id=\"the_end\"></a>\n",
    "# <p style=\"background-color:#FF6961; font-family:newtimeroman;color:#FFF9ED; font-size:150%; text-align:center; border-radius: 15px 50px;\"> ‚ú® THE END ‚ú®</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de50dd",
   "metadata": {
    "id": "links_bottom"
   },
   "source": [
    "<a href=\"https://nbviewer.org/github/huseyincenik/llm-from-scratch/blob/main/1_tokenizers/tokenizers.ipynb\"><img align=\"right\" src=\"https://i.ibb.co/48wtV8c/nbviewer-badge.png\" alt=\"Open in nbviewer\" width=\"130\" height=\"200\" title=\"Open and Execute in nbviewer\"></a><br/>\n",
    "\n",
    "<ul class=\"nav flex-column\">\n",
    "  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\" style=\"background-color:#FF6961; color:white\"><b> ‚úÇÔ∏è Tokenizers üî° </b></h3>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:red\" href=\"https://github.com/huseyincenik/llm-from-scratch/blob/main/1_tokenizers/tokenizers.ipynb\" target=\"_blank\">\n",
    "        <b>Github Notebook Link</b>\n",
    "      </a>\n",
    "      <img src=\"https://cdn-icons-png.flaticon.com/512/25/25231.png\" alt=\"GitHub\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:blue\" href=\"https://www.kaggle.com/code/huseyincenik/tokenizers\" target=\"_blank\">\n",
    "        <b>Kaggle Notebook Link</b>\n",
    "      </a>\n",
    "      <img src=\"https://cdn4.iconfinder.com/data/icons/logos-and-brands/512/189_Kaggle_logo_logos-512.png\" alt=\"Kaggle\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "  <li class=\"nav-item\">\n",
    "    <div style=\"display: flex; align-items: center;\">\n",
    "      <a class=\"nav-link\" style=\"background-color:; color:green\" href=\"https://www.linkedin.com/in/huseyincenik/\" target=\"_blank\">\n",
    "        <b>My Linkedin Account</b>\n",
    "      </a>\n",
    "      <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/LinkedIn_logo_initials.png/640px-LinkedIn_logo_initials.png\" alt=\"LinkedIn\" width=\"20\" height=\"20\" />\n",
    "    </div>\n",
    "  </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}